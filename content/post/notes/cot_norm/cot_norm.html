---
title: "Norms and Penalized Regression"
authors:
  - admin
categories: ["Explanatory Notes"]
tags: ["math","statistics","regression"]
date: 2018-07-23
summary: "Some notes on the math behind penalized regression methods liek Ridge and LASSO"
---



<div id="ridge-regression-and-the-lasso" class="section level1">
<h1>Ridge Regression and the Lasso</h1>
<p>While working through <a href="http://timothykylethomas.me/islr.html#islr">Introduction to Statistical Learning</a> (ISLR), I became intrigued by the lecture on <a href="https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/model_selection.pdf">subset selection</a>. Subset selection is the process of reducing linear models to only those features (viz. independent variables) that are most relevant. Old standbys in this field are piecewise forward and backward selection. The hot topic now, however, are the shrinkage methods ridge regression and lasso.</p>
<p>This two methods are popular now because of the explosion of higher dimensional data (i.e., where there are more features than observations). Piecewise selection frequently becomes intractable where there are many features. Shrinkage methods solve this method by building in a penalty to OLS estimation that automatically pushes coefficients to zero (you should read the above notes on subset selection for more details, here we are just plowing ahead with my chain of thought). The point of having less features and lower coefficients is the reduction in the overall variance of the model.</p>
<p>Let’s briefly overview OLS estimation and the changes the ridge and lasso make. With a vector <span class="math inline">\(b\)</span> of responses and a matrix <span class="math inline">\(A\)</span> of features, we seek to find the vector <span class="math inline">\(x\)</span> that makes <span class="math inline">\(Ax = b\)</span>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> Using standard terms for OLS, <span class="math inline">\(b\)</span> is the dependent variable <span class="math inline">\(y\)</span>, <span class="math inline">\(A\)</span> is our observations <span class="math inline">\(X\)</span>, and <span class="math inline">\(x\)</span> is the vector of <span class="math inline">\(\beta\)</span>s giving use the formula <span class="math inline">\(X \beta = y\)</span> (note that the order of <span class="math inline">\(X\)</span> and <span class="math inline">\(\beta\)</span> is important). The simplest way to express this is from a geometric perspective: we want to choose a <span class="math inline">\(\beta\)</span> vector that minimizes the distance between <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span> (the bars <span class="math inline">\(||\)</span> tell us that this is a euclidean norm — a fancy way of saying a distance):</p>
<p><span class="math display">\[\hat{\beta} = \min_{\beta} ||y - X\beta||. \]</span></p>
<p>The OLS way to do this is to minimize the sum of the squared residuals (RSS). There are many ways to express the minimization of RSS (a la the geometric version above), but here we will use the version presented in ISLR to remain consistent.</p>
<p>First, the RSS we seek to minimize is:</p>
<p><span class="math display">\[ RSS = \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 .\]</span></p>
<p>This is probably the most complicated way to present OLS, but in doing so, it quite exactly tells you what is happening.</p>
<p>For ridge regression we add a shrinkage penalty with tuning parameter <span class="math inline">\(\lambda\)</span> to the criterion:</p>
<p><span class="math display">\[ RSS + \lambda \sum_{j=1}^p \beta^2_j .\]</span></p>
<p>Now, instead of just trying to minimize RSS, we are trying to minimize a larger number: RSS plus the shrinkage penalty. This will force the model to prefer <span class="math inline">\(\beta\)</span>s that are smaller and closer to zero thus helping to reduce the variance of the model (you will often end up with very small coefficients and not ones exactly equal to zero).</p>
<p>The <span class="math inline">\(\sum \beta^2\)</span> part of the penalty is also known as the <span class="math inline">\(\ell_2\)</span> penalty and can be written in its geometric form as <span class="math inline">\(||\beta|\vert_2\)</span>.</p>
<p>A newer sibling of the ridge regression that is becoming increasingly more popular is the lasso:</p>
<p><span class="math display">\[ RSS + \lambda \sum_{j=1}^p |\beta_j |\]</span></p>
<p>By recasting the penalty as an <span class="math inline">\(\ell_1\)</span> norm (alternatively <span class="math inline">\(||\beta|\vert_1\)</span>), the minimization can take coefficients to zero (not just close to zero as in ridge regression). The result is feature selection: only those coefficients that are non-zero are kept. The disadvantage of the lasso is that it requires computationally intensive numerical searching algorithms.</p>
<p>So the difference between ridge and lasso is the nature of this penalty: ridge uses the <span class="math inline">\(\ell_2\)</span> norm while the lasso uses the <span class="math inline">\(\ell_1\)</span> norm (the <span class="math inline">\(\LaTeX\)</span> code for <span class="math inline">\(\ell\)</span> is <code>\ell</code>). So what are these norms?</p>
</div>
<div id="norm" class="section level1">
<h1>Norm</h1>
<p>When researching norms — particularly in the context of optimization problems like those used in subset selection — I came across this helpful <a href="https://rorasa.wordpress.com/2012/05/13/l0-norm-l1-norm-l2-norm-l-infinity-norm/">blog post</a>.</p>
<p>There is a lot to unpack from this post and many avenues of thoughts. One could easily start exploring the nature of Euclidean space and its alternatives (i.e., <a href="https://en.wikipedia.org/wiki/Hyperbolic_space">hyperbolic space</a>) and the variety of <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)">norms</a>. However, I decided to keep to the <span class="math inline">\(\ell_1\)</span> and <span class="math inline">\(\ell_2\)</span> norms, their optimization, and their application to the statistical issues presented above.</p>
<p>A simple way to think of a norm is a distance or size in a p-dimensional space. As the author of the blog writes, the general form of a norm is:</p>
<p><span class="math display">\[||x|\vert_p = \sqrt[p]{\sum_i |x_i|^p} \]</span></p>
<p>The <span class="math inline">\(\ell_1\)</span> norm ends up simply being the sum of the absolute value of each element, while the <span class="math inline">\(\ell_2\)</span> norm is the square root of the sum of the squared elements.</p>
<p>You may remember from grade school the formula for the distance (<span class="math inline">\(d\)</span>) between two points on the 2-dimensional plane <span class="math inline">\((x_1,y_2)\)</span> and <span class="math inline">\((x_2,y_2)\)</span>:</p>
<p><span class="math display">\[ d = \sqrt{(x_1-x_2)^2 + (y_1 - y_2)^2} \]</span></p>
<p>Which we can rewrite as:</p>
<p><span class="math display">\[|| y - x|\vert_2 = \sqrt{\sum (y - x)^2} \]</span></p>
<p>Does the above remind you of our minimization problem <span class="math inline">\(\hat{\beta} = \min_{\beta} ||y - X\beta||\)</span> or the RSS formula? It should because, as stated above, the OLS optimization is simply trying to find the <span class="math inline">\(\beta\)</span>s that minimizes the distance between <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span>. In other words, OLS wants the vector of <span class="math inline">\(\beta\)</span>s that makes the <span class="math inline">\(\ell_2\)</span> norm <span class="math inline">\(||y - X\beta||\)</span> the smallest. OLS is just a more multi-faceted version of something you have been doing all along.</p>
</div>
<div id="norm-optimization" class="section level1">
<h1>Norm Optimization</h1>
<p>To tie it together: regular OLS is simply minimizing an <span class="math inline">\(\ell_2\)</span> norm, ridge regression is minimizing an <span class="math inline">\(\ell_2\)</span> norm plus an <span class="math inline">\(\ell_2\)</span> penalty, and the lasso is minimizing an <span class="math inline">\(\ell_2\)</span> norm plus an <span class="math inline">\(\ell_1\)</span> norm.</p>
<p>Minimizing the <span class="math inline">\(\ell_1\)</span> norm (or any optimization problem involving the <span class="math inline">\(\ell_1\)</span> norm) requires the use of various numerical analysis <a href="https://www.cs.ubc.ca/~schmidtm/Documents/2005_Notes_Lasso.pdf">algorithms</a>. These were computationally unfeasible until recent developments in computing power.</p>
<p>Minimizing the <span class="math inline">\(\ell_2\)</span> norm is a more simple prospect. The author of the blog presents a way of doing this involving Lagrangian multipliers that ultimately dumps us off in OLS land. First, note that our objective function is</p>
<p><span class="math display">\[ \min ||x|\vert_2 \text{ s.t. } Ax = b  \]</span></p>
<p>The Lagrangian for this is:</p>
<p><span class="math display">\[ ||x||^2_2 + \lambda&#39; (Ax - b)\]</span></p>
<p>After solving<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>, the minimized estimate of x is <span class="math inline">\(A&#39; (AA&#39;)^{-1} b\)</span>. The ‘a-ha’ moment for me, was recognizing that this is in fact the same as <span class="math inline">\(\hat{\beta} = (X&#39;X)^{-1}X&#39;y\)</span>. The <span class="math inline">\((X&#39;X)^{-1}X&#39;\)</span> is known as the <a href="https://en.wikipedia.org/wiki/Moore–Penrose_pseudoinverse">Moore-Penrose Pseudoinverse Matrix</a>.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://en.wikipedia.org/wiki/Tikhonov_regularization">Tikhonov Regularization</a><a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>The mechanics of this process in the context of OLS (called constrained least squares) can be seen <a href="http://stanford.edu/class/ee103/lectures/constrained-least-squares/constrained-least-squares_slides.pdf">here</a><a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
